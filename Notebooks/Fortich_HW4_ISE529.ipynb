{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Madison-Taylor Fortich  \n",
    "9291000361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing the data:\n",
    "a. Import the libraries and datasets    \n",
    "b. Use ‘re’ library to extract the characters [a-zA-Z]  \n",
    "c. Use ‘nltk’ library to extract to stem the sentence and remove the stop words.   \n",
    "d. Leverage countvectorizer to vectorize the data.  \n",
    "e. Split the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'Restaurant_Reviews.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/madi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust not good',\n",
       " 'not tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price',\n",
       " 'get angri want damn pho',\n",
       " 'honeslti tast fresh',\n",
       " 'potato like rubber could tell made ahead time kept warmer',\n",
       " 'fri great',\n",
       " 'great touch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only printing the first 10\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1\n",
      " 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
      " 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
      " 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
      " 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
      " 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1\n",
      " 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1\n",
      " 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1] [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the models: \n",
    "a. Build Logistic Regression Classifier, Naive Bayes, Decision Tree Classifier, Random Forest Classifier.**   \n",
    "b. Use Grid Search CV to find the best parameters of each model, and train the models with the best parameters.**   \n",
    "c. Predict with the test set**  \n",
    "  \n",
    "### 3. Evaluation: \n",
    "a. Calculate the classification_report and confusion matrix of each model.  \n",
    "b. Write a brief report evaluating those models.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier_lr = LogisticRegression(random_state=0)\n",
    "classifier_lr.fit(X_train, y_train)\n",
    "y_pred_lr = classifier_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test Set Prediction: [0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0\n",
      " 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0\n",
      " 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_params = {'C': [0.1, 1, 10, 100], 'penalty': ['l2'], 'solver': ['lbfgs', 'liblinear']}\n",
    "\n",
    "grid_search_lr = GridSearchCV(estimator=classifier_lr, param_grid=lr_params, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_pred_lr_gs = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_search_lr.best_params_)\n",
    "print(\"Test Set Prediction:\", y_pred_lr_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78        97\n",
      "           1       0.82      0.73      0.77       103\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.77       200\n",
      "weighted avg       0.78      0.78      0.77       200\n",
      "\n",
      "Logistic Regression - Confusion Matrix\n",
      "[[80 17]\n",
      " [28 75]]\n",
      "Logistic Regression Grid Search - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78        97\n",
      "           1       0.82      0.73      0.77       103\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.77       200\n",
      "weighted avg       0.78      0.78      0.77       200\n",
      "\n",
      "Logistic Regression Grid Search - Confusion Matrix\n",
      "[[80 17]\n",
      " [28 75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Logistic Regression - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"Logistic Regression - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"Logistic Regression Grid Search - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_lr_gs))\n",
    "\n",
    "print(\"Logistic Regression Grid Search - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_lr_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Grid Search did not improve the Logistic Regression model. The accuracy remained the same at 78% as well as the classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier_nb = GaussianNB()\n",
    "classifier_nb.fit(X_train, y_train)\n",
    "y_pred_nb = classifier_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Naive Bayes): {'var_smoothing': 1e-05}\n",
      "Test Set Prediction: [1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "nb_params = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "\n",
    "grid_search_nb = GridSearchCV(estimator=classifier_nb, param_grid=nb_params, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "best_nb = grid_search_nb.best_estimator_\n",
    "y_pred_nb_gs = best_nb.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters (Naive Bayes):\", grid_search_nb.best_params_)\n",
    "print(\"Test Set Prediction:\", y_pred_nb_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.67        97\n",
      "           1       0.68      0.88      0.77       103\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.75      0.73      0.72       200\n",
      "weighted avg       0.75      0.73      0.72       200\n",
      "\n",
      "Naive Bayes - Confusion Matrix\n",
      "[[55 42]\n",
      " [12 91]]\n",
      "Naive Bayes Grid Search - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.58      0.68        97\n",
      "           1       0.69      0.88      0.77       103\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.76      0.73      0.73       200\n",
      "weighted avg       0.75      0.73      0.73       200\n",
      "\n",
      "Naive Bayes Grid Search - Confusion Matrix\n",
      "[[56 41]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "print(\"Naive Bayes - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "\n",
    "print(\"Naive Bayes Grid Search - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_nb_gs))\n",
    "\n",
    "print(\"Naive Bayes Grid Search - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_nb_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid Search improved the Naive Bayes model by a small margin. The accuracy is the same at 73%, but the True Negatives and False Positivws improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_tree = DecisionTreeClassifier(random_state=0)\n",
    "classifier_tree.fit(X_train, y_train)\n",
    "y_pred_tree = classifier_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Decision Tree): {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
      "Test Set Prediction: [0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0\n",
      " 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
      " 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
      " 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "tree_params = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, None], 'min_samples_split': [2, 10, 20]}\n",
    "\n",
    "grid_search_tree = GridSearchCV(estimator=classifier_tree, param_grid=tree_params, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "best_tree = grid_search_tree.best_estimator_\n",
    "y_pred_tree_gs = best_tree.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters (Decision Tree):\", grid_search_tree.best_params_)\n",
    "print(\"Test Set Prediction:\", y_pred_tree_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74        97\n",
      "           1       0.78      0.68      0.73       103\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.74      0.74      0.73       200\n",
      "weighted avg       0.74      0.73      0.73       200\n",
      "\n",
      "Decision Tree - Confusion Matrix\n",
      "[[77 20]\n",
      " [33 70]]\n",
      "Decision Tree Grid Search - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76        97\n",
      "           1       0.81      0.66      0.73       103\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.75      0.75      0.74       200\n",
      "weighted avg       0.76      0.74      0.74       200\n",
      "\n",
      "Decision Tree Grid Search - Confusion Matrix\n",
      "[[81 16]\n",
      " [35 68]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "print(\"Decision Tree - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "\n",
    "print(\"Decision Tree Grid Search - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_tree_gs))\n",
    "\n",
    "print(\"Decision Tree Grid Search - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_tree_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid Search improved the Decision Tree model by 1% in accuracy (73 to 74 percent). True Negatives and false positives also improved. Overall, there is some imporvement, but not by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_rf = RandomForestClassifier(random_state=0)\n",
    "classifier_rf.fit(X_train, y_train)\n",
    "y_pred_rf = classifier_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Random Forest): {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Test Set Prediction: [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators': [10, 50, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, None], \n",
    "             'min_samples_split': [2, 10, 20]}\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=classifier_rf, param_grid=rf_params, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf_gs = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters (Random Forest):\", grid_search_rf.best_params_)\n",
    "print(\"Test Set Prediction:\", y_pred_rf_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80        97\n",
      "           1       0.91      0.62      0.74       103\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.81      0.78      0.77       200\n",
      "weighted avg       0.81      0.78      0.77       200\n",
      "\n",
      "Random Forest - Confusion Matrix\n",
      "[[91  6]\n",
      " [39 64]]\n",
      "Random Forest Grid Search - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.94      0.79        97\n",
      "           1       0.91      0.60      0.73       103\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.80      0.77      0.76       200\n",
      "weighted avg       0.80      0.77      0.76       200\n",
      "\n",
      "Random Forest Grid Search - Confusion Matrix\n",
      "[[91  6]\n",
      " [41 62]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Random Forest - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Random Forest Grid Search - Classification Report\")\n",
    "print(classification_report(y_test, y_pred_rf_gs))\n",
    "\n",
    "print(\"Random Forest Grid Search - Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred_rf_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid Search didn't improve the Random Forest model. The accuracy decreased from 78 to 77%. There was also a decrease in precision, recall, and f-1 scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
